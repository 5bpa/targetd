#!/usr/bin/env python

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
# Copyright 2012, Andy Grover <agrover@redhat.com>
#
# A server that exposes a network interface for the LIO
# kernel target.

import os
import sys
import contextlib
import setproctitle
from rtslib import (Target, TPG, NodeACL, FabricModule, BlockStorageObject,
                    NetworkPortal, LUN, MappedLUN, RTSLibError, RTSLibNotInCFS)
import lvm
import json
from BaseHTTPServer import BaseHTTPRequestHandler, HTTPServer
from SocketServer import ThreadingMixIn
import socket
from threading import Lock
import yaml
import time
from targetcli import UIRoot
from configshell import ConfigShell
import tlslite
from subprocess import Popen, PIPE
import traceback


# Notes:
#
# User can configure block pools (lvm volume groups) 1 to many or 0-many file
# system mount points to be used as pools.  At this time you have to specify
# a block pool for block operations and file system mount point pool for FS
# operations.  We could use files on a file system for block too and create
# file systems on top of lvm too, but that is TBD.
#
# We are using btrfs to provide all the cool fast FS features.  User supplies a
# btrfs mount point and we create a targetd_fs and targetd_ss subvolumes.  Each
# time the user creates a file system we are creating a subvolume under fs.
# Each time a FS clone is made we create the clone under fs.  For each snapshot
# (RO clone) we are creating a read only snapshot in
# <mount>/ss/<fsname>/<snapshot name>
#
# There maybe better ways of utilizing btrfs,  we shall see.

setproctitle.setproctitle("targetd")

config_path = "/etc/target/targetd.yaml"

fs_path = "targetd_fs"
ss_path = "targetd_ss"
fs_cmd = 'btrfs'

error_codes = (-10,)

default_config = dict(
    #TODO: It would be good to do uuid verification before changing storage
    pools = { 'vg-targetd' : { 'uuid':None, 'type':'block'}},
    user = "admin",
    password = "targetd",
    target_name = "iqn.2003-01.org.linux-iscsi.%s:targetd" % socket.gethostname(),
    ssl = False,
    ssl_cert = "/etc/target/targetd_cert.pem",
    ssl_key = "/etc/target/targetd_key.pem",
)


class TargetdError(Exception):
    def __init__(self, error_code, message, *args, **kwargs):
        Exception.__init__(self, *args, **kwargs)
        self.error = error_code
        self.msg = message


def invoke(cmd, raise_exception=True):
    """
    Exec a command returning a tuple (exit code, stdout, stderr) and optionally
    throwing an exception on non-zero exit code.
    """
    c = Popen(cmd, stdout=PIPE, stderr=PIPE)
    out = c.communicate()

    if raise_exception:
        if c.returncode != 0:
            cmd_str = str(cmd)
            raise TargetdError(-303, 'Unexpected exit code "%s" %s, out= %s' %
                                     (cmd_str, str(c.returncode),
                                     str(out[0] + out[1])))

    return c.returncode, out[0], out[1]


def create_sub_volume(p):
    if not os.path.exists(p):
        invoke([fs_cmd, 'subvolume', 'create', p])

config = {}
if os.path.isfile(config_path):
    config = yaml.safe_load(open(config_path).read())
    if config is None:
        config = {}

for key, value in default_config.iteritems():
    if key not in config:
        config[key] = value

if os.getuid() != 0:
    print "targetd must run as root."
    sys.exit(-1)

# fail early if can't access vg
for k, v in config['pools'].items():
    if v['type'] == "block":
        test_vg = lvm.vgOpen(k)
        test_vg.close()
    else:
        #Make sure we have the appropriate subvolumes available
        dirs = [os.path.join(k, fs_path), os.path.join(k, ss_path)]
        for d in dirs:
            try:
                create_sub_volume(d)
            except TargetdError, e:
                print 'Unable to create required subvolumes!\n', e.msg
                raise

#
# Helper function to check/close vg for us.
#
@contextlib.contextmanager
def vgopen(pool_name):
    pool_check(pool_name)
    with contextlib.closing(lvm.vgOpen(pool_name, "w")) as vg:
        yield vg


def split_stdout(out):
    """
    Split the text out as an array of text arrays.
    """
    rc = []
    for line in out.split('\n'):
        elem = line.split(' ')
        if len(elem) > 1:
            rc.append(elem)
    return rc

def fs_space_values(mount_point):
    """
    Return a tuple (total, free) from the specified path
    """
    st = os.statvfs(mount_point)
    free = (st.f_bavail * st.f_frsize)
    total = (st.f_blocks * st.f_frsize)
    return total, free

def pool_check(pool_name):
    """
    pool_name *cannot* be trusted, funcs taking a pool param must call
    this or vgopen() to ensure passed-in pool name is one targetd has
    been configured to use.
    """
    if pool_name not in config['pools']:
        raise TargetdError(-110, "Invalid pool")

def volumes(req, pool):
    output = []
    with vgopen(pool) as vg:
        for lv in vg.listLVs():
            output.append(dict(name=lv.getName(), size=lv.getSize(),
                               uuid=lv.getUuid()))
    return output

def create(req, pool, name, size):
    with vgopen(pool) as vg:
        lv = vg.createLvLinear(name, int(size))

def fs_create(req, pool_name, name, size_bytes):
    pool_check(pool_name)

    if config['pools'][pool_name]['type'] == 'fs':
        full_path = os.path.join(pool_name, fs_path, name)

        if not os.path.exists(full_path):
            invoke([fs_cmd, 'subvolume', 'create', full_path])
        else:
            raise TargetdError(-53, 'FS already exists')
    else:
        raise TargetdError(-110, "Pool not appropriate for FS creation")


def fs_snapshot(req, fs_uuid, dest_ss_name):
    fs = _get_fs_by_uuid(req, fs_uuid)

    if fs:
        source_path = os.path.join(fs['pool'], fs_path, fs['name'])
        dest_base = os.path.join(fs['pool'], ss_path, fs['name'])
        dest_path = os.path.join(dest_base, dest_ss_name)

        create_sub_volume(dest_base)

        if os.path.exists(dest_path):
            raise TargetdError(-53, "Snapshot already exists with that name!")

        invoke([fs_cmd, 'subvolume', 'snapshot', '-r', source_path, dest_path])
    return None


def fs_snapshot_delete(req, fs_uuid, ss_uuid):
    fs_hash = _get_fs_by_uuid(req, fs_uuid)
    snapshot = _get_ss_by_uuid(req, fs_uuid, ss_uuid, fs_hash)
    path = os.path.join(fs_hash['pool'], ss_path, fs_hash['name'],
                        snapshot['name'])
    fs_subvolume_delete(path)

def destroy(req, pool, name):
    try:
        fm = FabricModule('iscsi')
        t = Target(fm, config['target_name'], mode='lookup')
        tpg = TPG(t, 1, mode='lookup')

        so_name = "%s:%s" % (pool, name)
        if so_name in (lun.storage_object.name for lun in tpg.luns):
            raise TargetdError(-303, "Volume '%s' cannot be "
                                     "removed while exported" % name)
    except RTSLibNotInCFS:
        pass

    with vgopen(pool) as vg:
        vg.lvFromName(name).remove()

def fs_subvolume_delete(path):
    invoke([fs_cmd, 'subvolume', 'delete', path])

def fs_destroy(req, pool, uuid, name):
    pool_check(pool)
    #Check to see if this file system has any read-only snapshots, if yes then
    #delete.  The API requires a FS to list its RO copies, we may want to
    #reconsider this decision.

    base_snapshot_dir = os.path.join(pool, ss_path, name)

    snapshots = ss(req, uuid)
    for s in snapshots:
        fs_subvolume_delete(os.path.join(base_snapshot_dir, s['name']))

    if os.path.exists(base_snapshot_dir):
        fs_subvolume_delete(base_snapshot_dir)

    fs_subvolume_delete(os.path.join(pool, fs_path, name))


def copy(req, pool, vol_orig, vol_new, timeout=10):
    """
    Create a new volume that is a copy of an existing one.
    If this operation takes longer than the timeout, it will return
    an async completion and report actual status via async_complete().
    """
    with vgopen(pool) as vg:
        copy_size = vg.lvFromName(vol_orig).getSize()

    create(req, pool, vol_new, copy_size)

    try:
        src_path = "/dev/%s/%s" % (pool, vol_orig)
        dst_path = "/dev/%s/%s" % (pool, vol_new)

        start_time = time.clock()
        with open(src_path, 'rb') as fsrc:
            with open(dst_path, 'wb') as fdst:
                copied = 0
                while copied != copy_size:
                    buf = fsrc.read(1024*1024)
                    if not buf:
                        break
                    fdst.write(buf)
                    copied += len(buf)
                    if time.clock() > (start_time + timeout):
                        req.async_completion()
                        async_status(req, 0, int((float(copied)/copy_size)*100))
        complete_if_async(req, 0)

    except Exception, e:
        destroy(req, pool, vol_new)
        raise TargetdError(-303, "Unexpected exception: %s" % (str(e)))


def export_list(req):
    try:
        fm = FabricModule('iscsi')
        t = Target(fm, config['target_name'], mode='lookup')
        tpg = TPG(t, 1, mode='lookup')
    except RTSLibNotInCFS:
        return []

    exports = []
    for na in tpg.node_acls:
        for mlun in na.mapped_luns:
            mlun_vg, mlun_name = mlun.tpg_lun.storage_object.udev_path.split("/")[2:]
            with vgopen(mlun_vg) as vg:
                lv = vg.lvFromName(mlun_name)
                exports.append(dict(initiator_wwn=na.node_wwn, lun=mlun.mapped_lun,
                                    vol_name=mlun_name, pool=mlun_vg,
                                    vol_uuid=lv.getUuid(), vol_size=lv.getSize()))
    return exports

#
# HACK: call targetcli saveconfig method to save state
#
def _exports_save_config():
    root = UIRoot(ConfigShell(), as_root=True)
    root.ui_command_saveconfig()

def export_create(req, pool, vol, initiator_wwn, lun):

    # get wwn of volume so LIO can export as vpd83 info
    with vgopen(pool) as vg:
        vol_serial = vg.lvFromName(vol).getUuid()

    # only add new SO if it doesn't exist
    # so.name concats pool & vol names separated by ':'
    so_name = "%s:%s" % (pool,vol)
    try:
        so = BlockStorageObject(so_name)
    except RTSLibError:
        so = BlockStorageObject(so_name, dev="/dev/%s/%s" % (pool, vol))
        so.wwn = vol_serial

    # export useful scsi model if kernel > 3.8
    try:
        so.set_attribute("emulate_model_alias", 1)
    except RTSLibError:
        pass

    fm = FabricModule('iscsi')
    t = Target(fm, config['target_name'])
    tpg = TPG(t, 1)
    tpg.enable = True
    tpg.set_attribute("authentication", '0')
    np = NetworkPortal(tpg, "0.0.0.0")
    na = NodeACL(tpg, initiator_wwn)

    # only add tpg lun if it doesn't exist
    for tmp_lun in tpg.luns:
        if tmp_lun.storage_object.name == so.name \
                and tmp_lun.storage_object.plugin == 'block':
            tpg_lun = tmp_lun
            break
    else:
        tpg_lun = LUN(tpg, storage_object=so)

    # only add mapped lun if it doesn't exist
    for tmp_mlun in tpg_lun.mapped_luns:
        if tmp_mlun.mapped_lun == lun:
            mapped_lun = tmp_mlun
            break
    else:
        mapped_lun = MappedLUN(na, lun, tpg_lun)

    _exports_save_config()

def export_destroy(req, pool, vol, initiator_wwn):
    pool_check(pool)
    fm = FabricModule('iscsi')
    t = Target(fm, config['target_name'])
    tpg = TPG(t, 1)
    na = NodeACL(tpg, initiator_wwn)

    for mlun in na.mapped_luns:
        # all SOs are Block so we can access udev_path safely
        mlun_vg, mlun_name = mlun.tpg_lun.storage_object.udev_path.split("/")[2:]

        if mlun_vg == pool and mlun_name == vol:
            tpg_lun = mlun.tpg_lun
            mlun.delete()
            # be tidy and delete unused tpg lun mappings?
            if not len(list(tpg_lun.mapped_luns)):
                so = tpg_lun.storage_object
                tpg_lun.delete()
                so.delete()
            break
    else:
        raise TargetdError(-151, "Volume '%s' not found in %s exports" %
                          (vol, initiator_wwn))

    # Clean up tree if branch has no leaf
    if not len(list(na.mapped_luns)):
        na.delete()
        if not len(list(tpg.node_acls)):
            tpg.delete()
            if not len(list(t.tpgs)):
                t.delete()

    _exports_save_config()


def pools(req):
    results = []

    for k,v in config['pools'].items():
        if v['type'] == 'block':
            with vgopen(k) as vg:
                results.append(dict(name=vg.getName(), size=vg.getSize(),
                                free_size=vg.getFreeSize(), type='block'))
        else:
            total, free = fs_space_values(k)
            results.append(dict(name=k, size=total, free_size=free, type='fs'))

    return results

def fs(req):
    fs = []

    for k, v in config['pools'].items():
        if v['type'] == 'fs':
            full_path = os.path.join(k, fs_path)

            #TODO take out this loop, used to handle bug in btrfs
            #ERROR: Failed to lookup path for root 0 - No such file or directory
            while True:
                result, out, err = invoke([fs_cmd, 'subvolume', 'list', '-u',
                                           full_path], False)
                if result == 0:
                    data = split_stdout(out)
                    if len(data):
                        (total, free) = fs_space_values(full_path)
                        for e in data:
                            fs.append(dict(name=e[10],uuid=e[8],
                                           total_space=total, free_space=free,
                                           pool=k))
                    break
                elif result == 19:
                    time.sleep(1)
                    continue
                else:
                    raise TargetdError(-303, "Unexpected exit code %d" % result)

    return fs


def ss(req, fs_uuid, fs_cache=None):
    snapshots = []

    if fs_cache is None:
        fs_cache = _get_fs_by_uuid(req, fs_uuid)

    full_path = os.path.join(fs_cache['pool'], ss_path, fs_cache['name'])

    #TODO take out this loop, used to handle bug in btrfs
    # ERROR: Failed to lookup path for root 0 - No such file or directory

    if os.path.exists(full_path):
        while True:
            result, out, err = invoke([fs_cmd, 'subvolume', 'list', '-s',
                                       full_path], False)
            if result == 0:
                data = split_stdout(out)
                if len(data):
                    (total, free) = fs_space_values(full_path)
                    for e in data:
                        ts = "%s %s" % (e[10], e[11])
                        time_epoch = int(time.mktime(
                            time.strptime(ts, '%Y-%m-%d %H:%M:%S')))
                        st = dict(name=e[-1], uuid=e[-3], timestamp=time_epoch)
                        snapshots.append(st)
                break
            elif result == 19:
                time.sleep(1)
                continue
            else:
                raise TargetdError(-303, "Unexpected exit code %d" % result)

    return snapshots


def _get_fs_by_uuid(req, fs_uuid):
    current_fs = fs(req)
    for f in current_fs:
        if f['uuid'] == fs_uuid:
            return f

    return None


def _get_ss_by_uuid(req, fs_uuid, ss_uuid, fs=None):

    if fs is None:
        fs = _get_fs_by_uuid(req, fs_uuid)

    snapshots = ss(req, fs_uuid, fs)

    for s in snapshots:
        if s['uuid'] == ss_uuid:
            return s
    return None

def fs_clone(req, fs_uuid, dest_fs_name, snapshot_id):
    fs = _get_fs_by_uuid(req, fs_uuid)

    if fs and not snapshot_id:
        base = os.path.join(fs['pool'], fs_path)
        dest = os.path.join(base, dest_fs_name)
        if os.path.exists(dest):
            raise TargetdError(-51, "Filesystem with that name exists!")

        invoke([fs_cmd, 'subvolume', 'snapshot', os.path.join(base, fs['name']),
                dest])
    elif fs and snapshot_id:
        snapshot = _get_ss_by_uuid(req, fs_uuid, snapshot_id)
        base = os.path.join(fs['pool'], fs_path)

        if snapshot:
            source = os.path.join(fs['pool'], ss_path, fs['name'], snapshot['name'])
            dest = os.path.join(base, dest_fs_name)

            if os.path.exists(dest):
                raise TargetdError(-51, "Filesystem with that name exists!")

            invoke([fs_cmd, 'subvolume', 'snapshot', source, dest])
        else:
            raise TargetdError(-112, "snapshot not found")
    else:
        raise TargetdError(-104, "fs_uuid not found!")

    return None

def async_list(req):
    """
    Return a list of ongoing processes. Processes that have terminated with an
    error are returned once and then delisted.
    """
    with long_op_status_lock:
        status_dict = long_op_status.copy()
        for key, item in long_op_status:
            if item[0] > 0:
                del long_op_status[key]
    return status_dict


async_id_lock = Lock()
async_id = 100

def new_async_id():
    global async_id
    with async_id_lock:
        new_id = async_id
        async_id += 1
    return new_id

# Long-running threads update their progress here
long_op_status_lock = Lock()
# async_id -> (code, pct_complete)
long_op_status = dict()

def async_status(req, code, pct_complete=None):
    """
    update a global array with status of ongoing ops.
    code: 0 if ok, or -err
    pct_complete: percent complete, integer 0-100
    """
    with long_op_status_lock:
        long_op_status[req.async_id] = (code, pct_complete)

def complete_if_async(req, code):
    """
    Ongoing op is done, remove status if succeeded
    """
    if req.async_id:
        with long_op_status_lock:
            if not code:
                del long_op_status[req.async_id]


mapping = dict(
    vol_list=volumes,
    vol_create=create,
    vol_destroy=destroy,
    vol_copy=copy,
    export_list=export_list,
    export_create=export_create,
    export_destroy=export_destroy,
    pool_list=pools,
    async_list=async_list,
    fs_list=fs,
    fs_destroy=fs_destroy,
    fs_create=fs_create,
    fs_clone=fs_clone,
    ss_list=ss,
    fs_snapshot=fs_snapshot,
    fs_snapshot_delete=fs_snapshot_delete
    )


class TargetHandler(BaseHTTPRequestHandler):

    def log_request(self, code='-', size='-'):
        # override base class - don't log good requests
        pass

    def do_POST(self):

        self.async_id = None
        rpcdata = ""
        error = None

        # get basic auth string, strip "Basic "
        try:
            auth64 = self.headers.getheader("Authorization")[6:]
            in_user, in_pass = auth64.decode('base64').split(":")
        except:
            self.send_error(400)
            return

        if in_user != config['user'] or in_pass != config['password']:
            self.send_error(401)
            return

        if not self.path == "/targetrpc":
            self.send_error(404)
            return

        try:
            error = (-1, "jsonrpc error")
            self.id = None
            try:
                content_len = int(self.headers.getheader('content-length'))
                req = json.loads(self.rfile.read(content_len))
            except ValueError:
                # see http://www.jsonrpc.org/specification for errcodes
                errcode = (-32700, "parse error")
                raise

            self.send_response(200)
            self.send_header("Content-type", "application/json")
            self.end_headers()

            try:
                version = req['jsonrpc']
                if version != "2.0":
                    raise ValueError
                method = req['method']
                self.id = int(req['id'])
                params = req.get('params', None)
            except (KeyError, ValueError):
                error = (-32600, "not a valid jsonrpc-2.0 request")
                raise

            try:
                if params:
                    result = mapping[method](self, **params)
                else:
                    result = mapping[method](self)
            except KeyError:
                error = (-32601, "method %s not found" % method)
                raise
            except TypeError:
                #print traceback.format_exc()
                error = (-32602, "invalid method parameter(s)")
                raise
            except TargetdError, td:
                error = (td.error, td.msg)
                raise
            except Exception, e:
                error = (-1, "%s: %s" % (type(e).__name__, e))
                raise

            rpcdata = json.dumps(dict(result=result, id=self.id))

        except:
            print 'Error data=', str(error)
            rpcdata = json.dumps(dict(error=dict(code=error[0],
                                                 message=error[1]), id=self.id))
        finally:
            if not self.async_id:
                self.wfile.write(rpcdata)

    def async_completion(self):
        if not self.async_id:
            self.async_id = new_async_id()
            rpcdata = json.dumps(dict(error=dict(code=self.async_id, message="Async Operation"), id=self.id))
            self.wfile.write(rpcdata)
            # wfile is buffered, need to do this to flush the response
            self.connection.shutdown(socket.SHUT_WR)

class ThreadedHTTPServer(ThreadingMixIn, HTTPServer, object):
    """Handle requests in a separate thread."""

class TLSThreadedHTTPServer(tlslite.TLSSocketServerMixIn, ThreadedHTTPServer):
    """Also use TLS to encrypt the connection"""

    def __init__(self, *args, **kwargs):
        super(TLSThreadedHTTPServer, self).__init__(*args, **kwargs)
        s = open(config['ssl_cert']).read()
        x509 = tlslite.X509()
        x509.parse(s)
        self.certChain = tlslite.X509CertChain([x509])

        s = open(config['ssl_key']).read()
        self.privateKey = tlslite.parsePEMKey(s, private=True)

        self.sessionCache = tlslite.SessionCache()

    def handshake(self, tlsConnection):
        try:
            tlsConnection.handshakeServer(certChain=self.certChain,
                                          privateKey=self.privateKey,
                                          sessionCache=self.sessionCache)
            tlsConnection.ignoreAbruptClose = True
            return True
        except tlslite.TLSError, error:
            print "Handshake failure:", str(error)
            return False


if __name__ == "__main__":
    server = None

    if config['ssl']:
        server_class = TLSThreadedHTTPServer
        note = "(TLS yes)"
    else:
        server_class = ThreadedHTTPServer
        note = "(TLS no)"

    try:
        server = server_class(('', 18700), TargetHandler)
        print "started server", note
        server.serve_forever()
    except KeyboardInterrupt:
        print "SIGINT received, shutting down"
        if server is not None:
            server.socket.close()
